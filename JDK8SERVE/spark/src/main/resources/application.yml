spring:
  application:
    name: spark-springboot
  redis:
    host: 127.0.0.1
    port: 6379
    database: 0
    timeout: 1800000 # 使用毫秒格式，兼容 Redis Lettuce
    password: Alone117
    lettuce:
      pool:
        max-active: 20  # 最大连接数
        max-wait: -1    # 最大阻塞等待时间(负数表示无限制)
        max-idle: 5     # 最大空闲连接数
        min-idle: 0     # 最小空闲连接数
server:
  error:
    whitelabel:
      enabled: false # 禁用 Spring 默认错误页面
  port: 8082

logging:
  level:
    org.springframework.boot.autoconfigure: OFF
    com.david.hlp.spark.service: ERROR
  file:
    name: /home/david/Project/BlogAndChart/logs/spark.log # 主日志文件路径
  logback:
    rollingpolicy:
      max-file-size: 10MB
      max-history: 30
    file:
      error: /home/david/Project/BlogAndChart/logs/spark-error.log # 错误日志文件路径
    pattern:
      console: "%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n"
      file: "%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n"
      error: "%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n"
    appender:
      error:
        class: ch.qos.logback.core.rolling.RollingFileAppender
        level: ERROR
        file: ${logging.logback.file.error}
        rollingPolicy:
          class: ch.qos.logback.core.rolling.TimeBasedRollingPolicy
          fileNamePattern: ${logging.logback.file.error}.%d{yyyy-MM-dd}
          maxHistory: 30
        encoder:
          pattern: ${logging.logback.pattern.error}
        filter:
          class: ch.qos.logback.classic.filter.ThresholdFilter
          level: ERROR

jackson:
  date-format: yyyy-MM-dd HH:mm:ss
  time-zone: Asia/Shanghai
  serialization:
    write-dates-as-timestamps: false
  deserialization:
    fail-on-unknown-properties: false

# Spark配置
spark:
  app:
    name: springboot-spark
  master:
    uri: local[2]
  redis:
    key-prefix: "spark:job:"
    host: localhost
    port: 6379
    password: Alone117
    timeout: 2000
    pool:
      max-total: 10
      max-idle: 5
      min-idle: 1
    data:
      expire-days: 7